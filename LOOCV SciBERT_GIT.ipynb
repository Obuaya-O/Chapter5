{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LOOCV: SciBERT embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pandas import read_csv\n",
    "\n",
    "import numpy as np\n",
    "from numpy import mean, std\n",
    "\n",
    "from sklearn.model_selection import LeaveOneOut\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "# Import Models\n",
    "\n",
    "from sklearn.metrics import precision_score, recall_score, accuracy_score, f1_score, confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import scipy as sp\n",
    "\n",
    "from flair.embeddings import TransformerWordEmbeddings\n",
    "from flair.embeddings import WordEmbeddings, FlairEmbeddings, DocumentPoolEmbeddings\n",
    "from flair.data import Sentence\n",
    "from flair import torch\n",
    "\n",
    "import umap\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import tqdm as notebook_tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "euct_df = pd.read_csv(euct_df) \n",
    "# Data already preprocess since classification tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LOOCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "euct_df['concat_corpus'] = euct_df['Title']+ \" \" + euct_df['Objective'] + \" \" + euct_df['pr_endpoint'] + \" \" + euct_df['endpoint_description']\n",
    "# euct_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill NaN values with an empty string\n",
    "euct_df['concat_corpus'] = euct_df['concat_corpus'].fillna('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pretrained SciBERT tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"allenai/scibert_scivocab_uncased\")\n",
    "model = AutoModel.from_pretrained(\"allenai/scibert_scivocab_uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the text and generate embeddings\n",
    "def generate_embeddings(texts, tokenizer, model, max_len=512):\n",
    "    \"\"\"Generate embeddings for a list of texts using SciBERT.\"\"\"\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    with torch.no_grad():\n",
    "        inputs = tokenizer(\n",
    "            texts.tolist(), \n",
    "            padding=True, \n",
    "            truncation=True, \n",
    "            max_length=max_len, \n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        outputs = model(**inputs)\n",
    "        # Use the [CLS] token representation (typically at index 0)\n",
    "        embeddings = outputs.last_hidden_state[:, 0, :].numpy()\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = euct_df['concat_corpus']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate embeddings for the EUCT-NS concat corpus\n",
    "X = generate_embeddings(a, tokenizer, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into X and y\n",
    "X = X\n",
    "y = euct_df['manual_label'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Initial DataFrame shape:\", euct_df.shape)\n",
    "euct_df.dropna(subset=['concat_corpus', 'manual_label'], inplace=True)  # Drop rows with NaN values in relevant columns\n",
    "print(\"Shape after dropping NaNs:\", euct_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Features shape: {X.shape}, Target shape: {y.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "if X.shape[0] != y.shape[0]:\n",
    "    raise ValueError(\"Features and target variable have inconsistent number of samples.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "# Define the parameters to search. Want to figure out the best parameters for the model\n",
    "parameters = {'parameters of the model'}\n",
    "\n",
    "mdl = MDL()\n",
    "pipeline = make_pipeline(StandardScaler(), mdl)\n",
    "mdl = GridSearchCV(pipeline, parameters, cv=10)\n",
    "mdl.fit(X, y)\n",
    "\n",
    "# Display the best parameters found by GridSearchCV\n",
    "print(\"Best parameters found: \", mdl.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "loo = LeaveOneOut()\n",
    "predictions = []\n",
    "actuals = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdl = MDL(#parameters of mdl, change these to the best parameters found by GridSearchCV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "for train_index, test_index in loo.split(X):\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "    mdl.fit(X_train, y_train)\n",
    "\n",
    "    y_pred = mdl.predict(X_test)\n",
    "\n",
    "    predictions.append(y_pred[0])\n",
    "    actuals.append(y_test[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "accuracy_weighted = accuracy_score(actuals, predictions)\n",
    "precision_weighted = precision_score(actuals, predictions, average='weighted')\n",
    "recall_weighted = recall_score(actuals, predictions, average='weighted')\n",
    "f1_weighted = f1_score(actuals, predictions, average='weighted')\n",
    "\n",
    "accuracy_unweighted = accuracy_score(actuals, predictions)\n",
    "classification_metrics = classification_report(actuals, predictions, output_dict=True)\n",
    "\n",
    "# Print metrics\n",
    "print(f'LOOCV Accuracy (Weighted): {accuracy_weighted:.2f}')\n",
    "print(f'LOOCV Precision (Weighted): {precision_weighted:.2f}')\n",
    "print(f'LOOCV Recall (Weighted): {recall_weighted:.2f}')\n",
    "print(f'LOOCV F1 Score (Weighted): {f1_weighted:.2f}')\n",
    "print()\n",
    "\n",
    "print(f'LOOCV Accuracy (Unweighted): {accuracy_unweighted:.2f}')\n",
    "print(\"Precision, Recall, and F1 Score by Class:\")\n",
    "for cls, metrics in classification_metrics.items():\n",
    "    if cls.isdigit():  # Filter class-specific metrics\n",
    "        print(f\"  Class {cls}: Precision={metrics['precision']:.2f}, Recall={metrics['recall']:.2f}, F1 Score={metrics['f1-score']:.2f}\")\n",
    "\n",
    "# Calculate mean and standard deviation of precision, recall, and F1 scores across classes\n",
    "class_precisions = [metrics['precision'] for cls, metrics in classification_metrics.items() if cls.isdigit()]\n",
    "class_recalls = [metrics['recall'] for cls, metrics in classification_metrics.items() if cls.isdigit()]\n",
    "class_f1_scores = [metrics['f1-score'] for cls, metrics in classification_metrics.items() if cls.isdigit()]\n",
    "\n",
    "print()\n",
    "print(f'Mean and Standard Deviation of Precision: Mean={np.mean(class_precisions):.2f}, Std={np.std(class_precisions):.2f}')\n",
    "print(f'Mean and Standard Deviation of Recall: Mean={np.mean(class_recalls):.2f}, Std={np.std(class_recalls):.2f}')\n",
    "print(f'Mean and Standard Deviation of F1 Score: Mean={np.mean(class_f1_scores):.2f}, Std={np.std(class_f1_scores):.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_matrix = confusion_matrix(actuals, predictions)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=conf_matrix, display_labels=rfc.classes_)\n",
    "disp.plot(cmap=plt.cm.Greens)\n",
    "plt.title('Confusion Matrix of Estimated Performance When Making Predictions On New Data Based on Concatenated Text')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Performed LOOCV with dimensionality reduction applied to the SciBERT embeddings to see if estimations improved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "umap_reducer = umap.UMAP(n_components=2, random_state=42) \n",
    "umap_result = umap_reducer.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = umap_result\n",
    "y = euct_df['manual_label'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Initial DataFrame shape:\", euct_df.shape)\n",
    "euct_df.dropna(subset=['concat_corpus', 'manual_label'], inplace=True)  # Drop rows with NaN values in relevant columns\n",
    "print(\"Shape after dropping NaNs:\", euct_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Features shape: {X.shape}, Target shape: {y.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "if X.shape[0] != y.shape[0]:\n",
    "    raise ValueError(\"Features and target variable have inconsistent number of samples.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {'parameters of the model'}\n",
    "\n",
    "mdl = MDL()\n",
    "pipeline = make_pipeline(StandardScaler(), mdl)\n",
    "mdl = GridSearchCV(pipeline, parameters, cv=10)\n",
    "mdl.fit(X, y)\n",
    "\n",
    "# Display the best parameters found by GridSearchCV\n",
    "print(\"Best parameters found: \", mdl.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "loo = LeaveOneOut()\n",
    "predictions = []\n",
    "actuals = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdl = MDL(#parameters of mdl, change these to the best parameters found by GridSearchCV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "for train_index, test_index in loo.split(X):\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "    mdl.fit(X_train, y_train)\n",
    "\n",
    "    y_pred = mdl.predict(X_test)\n",
    "\n",
    "    predictions.append(y_pred[0])\n",
    "    actuals.append(y_test[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_weighted = accuracy_score(actuals, predictions)\n",
    "precision_weighted = precision_score(actuals, predictions, average='weighted')\n",
    "recall_weighted = recall_score(actuals, predictions, average='weighted')\n",
    "f1_weighted = f1_score(actuals, predictions, average='weighted')\n",
    "\n",
    "accuracy_unweighted = accuracy_score(actuals, predictions)\n",
    "classification_metrics = classification_report(actuals, predictions, output_dict=True)\n",
    "\n",
    "# Print metrics\n",
    "print(f'LOOCV Accuracy (Weighted): {accuracy_weighted:.2f}')\n",
    "print(f'LOOCV Precision (Weighted): {precision_weighted:.2f}')\n",
    "print(f'LOOCV Recall (Weighted): {recall_weighted:.2f}')\n",
    "print(f'LOOCV F1 Score (Weighted): {f1_weighted:.2f}')\n",
    "print()\n",
    "\n",
    "print(f'LOOCV Accuracy (Unweighted): {accuracy_unweighted:.2f}')\n",
    "print(\"Precision, Recall, and F1 Score by Class:\")\n",
    "for cls, metrics in classification_metrics.items():\n",
    "    if cls.isdigit():  # Filter class-specific metrics\n",
    "        print(f\"  Class {cls}: Precision={metrics['precision']:.2f}, Recall={metrics['recall']:.2f}, F1 Score={metrics['f1-score']:.2f}\")\n",
    "\n",
    "# Calculate mean and standard deviation of precision, recall, and F1 scores across classes\n",
    "class_precisions = [metrics['precision'] for cls, metrics in classification_metrics.items() if cls.isdigit()]\n",
    "class_recalls = [metrics['recall'] for cls, metrics in classification_metrics.items() if cls.isdigit()]\n",
    "class_f1_scores = [metrics['f1-score'] for cls, metrics in classification_metrics.items() if cls.isdigit()]\n",
    "\n",
    "print()\n",
    "print(f'Mean and Standard Deviation of Precision: Mean={np.mean(class_precisions):.2f}, Std={np.std(class_precisions):.2f}')\n",
    "print(f'Mean and Standard Deviation of Recall: Mean={np.mean(class_recalls):.2f}, Std={np.std(class_recalls):.2f}')\n",
    "print(f'Mean and Standard Deviation of F1 Score: Mean={np.mean(class_f1_scores):.2f}, Std={np.std(class_f1_scores):.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_matrix = confusion_matrix(actuals, predictions)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=conf_matrix, display_labels=rfc.classes_)\n",
    "disp.plot(cmap=plt.cm.Greens)\n",
    "plt.title('Confusion Matrix of Estimated Performance When Making Predictions On New Data Based on Concatenated Text with UMAP')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "clustering_endpoints",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
